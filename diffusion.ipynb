{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "940c8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew,kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c31ad19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9712e5e44104066b116145f7efc27e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# after a lot of training\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m sampled_images \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m sampled_images\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:683\u001b[0m, in \u001b[0;36mGaussianDiffusion.sample\u001b[0;34m(self, batch_size, return_all_timesteps)\u001b[0m\n\u001b[1;32m    681\u001b[0m image_size, channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels\n\u001b[1;32m    682\u001b[0m sample_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_sample_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_ddim_sampling \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mddim_sample\n\u001b[0;32m--> 683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_all_timesteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:629\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop\u001b[0;34m(self, shape, return_all_timesteps)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)), desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling loop time step\u001b[39m\u001b[38;5;124m'\u001b[39m, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps):\n\u001b[1;32m    628\u001b[0m     self_cond \u001b[38;5;241m=\u001b[39m x_start \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_condition \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     img, x_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     imgs\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m    632\u001b[0m ret \u001b[38;5;241m=\u001b[39m img \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_all_timesteps \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(imgs, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:613\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[0;34m(self, x, t, x_self_cond)\u001b[0m\n\u001b[1;32m    611\u001b[0m b, \u001b[38;5;241m*\u001b[39m_, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape, x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    612\u001b[0m batched_times \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((b,), t, device \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m--> 613\u001b[0m model_mean, _, model_log_variance, x_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatched_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_self_cond\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_self_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x) \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;66;03m# no noise if t == 0\u001b[39;00m\n\u001b[1;32m    615\u001b[0m pred_img \u001b[38;5;241m=\u001b[39m model_mean \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m model_log_variance)\u001b[38;5;241m.\u001b[39mexp() \u001b[38;5;241m*\u001b[39m noise\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:600\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, x, t, x_self_cond, clip_denoised)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_mean_variance\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t, x_self_cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, clip_denoised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 600\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_self_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     x_start \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mpred_x_start\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clip_denoised:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:575\u001b[0m, in \u001b[0;36mGaussianDiffusion.model_predictions\u001b[0;34m(self, x, t, x_self_cond, clip_x_start, rederive_pred_noise)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t, x_self_cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, clip_x_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, rederive_pred_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 575\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_self_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     maybe_clip \u001b[38;5;241m=\u001b[39m partial(torch\u001b[38;5;241m.\u001b[39mclamp, \u001b[38;5;28mmin\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m clip_x_start \u001b[38;5;28;01melse\u001b[39;00m identity\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_noise\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:391\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, time, x_self_cond)\u001b[0m\n\u001b[1;32m    389\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, h\u001b[38;5;241m.\u001b[39mpop()), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    390\u001b[0m     x \u001b[38;5;241m=\u001b[39m block2(x, t)\n\u001b[0;32m--> 391\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     x \u001b[38;5;241m=\u001b[39m upsample(x)\n\u001b[1;32m    395\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, r), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:86\u001b[0m, in \u001b[0;36mResidual.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:134\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    133\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/BSc/lib/python3.10/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:234\u001b[0m, in \u001b[0;36mLinearAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb (h c) x y -> b h c (x y)\u001b[39m\u001b[38;5;124m'\u001b[39m, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads), qkv)\n\u001b[1;32m    233\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39msoftmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 234\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    237\u001b[0m v \u001b[38;5;241m=\u001b[39m v \u001b[38;5;241m/\u001b[39m (h \u001b[38;5;241m*\u001b[39m w)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8)\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 1000,   # number of steps\n",
    "    loss_type = 'l1'    # L1 or L2\n",
    ")\n",
    "\n",
    "training_images = torch.rand(8, 3, 128, 128) # images are normalized from 0 to 1\n",
    "loss = diffusion(training_images)\n",
    "loss.backward()\n",
    "# after a lot of training\n",
    "\n",
    "sampled_images = diffusion.sample(batch_size = 4)\n",
    "Ã§sampled_images.shape # (4, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c43e4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0259, 0.2036, 0.9734],\n",
       "          [0.1802, 0.0739, 0.8595]],\n",
       "\n",
       "         [[0.2924, 0.4906, 0.6220],\n",
       "          [0.3739, 0.4063, 0.7581]],\n",
       "\n",
       "         [[0.0289, 0.8885, 0.0247],\n",
       "          [0.0993, 0.3566, 0.0205]]],\n",
       "\n",
       "\n",
       "        [[[0.4837, 0.4903, 0.5456],\n",
       "          [0.3821, 0.3478, 0.3806]],\n",
       "\n",
       "         [[0.8102, 0.5793, 0.8773],\n",
       "          [0.5361, 0.8563, 0.5779]],\n",
       "\n",
       "         [[0.2361, 0.2200, 0.2607],\n",
       "          [0.5157, 0.7281, 0.2545]]],\n",
       "\n",
       "\n",
       "        [[[0.6991, 0.8626, 0.5721],\n",
       "          [0.9277, 0.8865, 0.4576]],\n",
       "\n",
       "         [[0.7727, 0.4941, 0.6689],\n",
       "          [0.0258, 0.4509, 0.9462]],\n",
       "\n",
       "         [[0.5269, 0.7109, 0.5977],\n",
       "          [0.8364, 0.2127, 0.7418]]],\n",
       "\n",
       "\n",
       "        [[[0.0997, 0.6199, 0.5599],\n",
       "          [0.0042, 0.7716, 0.8174]],\n",
       "\n",
       "         [[0.8834, 0.5592, 0.8820],\n",
       "          [0.4094, 0.7939, 0.9521]],\n",
       "\n",
       "         [[0.7895, 0.0728, 0.8421],\n",
       "          [0.2545, 0.0628, 0.1603]]],\n",
       "\n",
       "\n",
       "        [[[0.7320, 0.6331, 0.0810],\n",
       "          [0.2317, 0.9325, 0.6698]],\n",
       "\n",
       "         [[0.9145, 0.0871, 0.3829],\n",
       "          [0.0917, 0.5108, 0.0347]],\n",
       "\n",
       "         [[0.1147, 0.1657, 0.7743],\n",
       "          [0.6938, 0.7467, 0.9609]]],\n",
       "\n",
       "\n",
       "        [[[0.7232, 0.7270, 0.9006],\n",
       "          [0.1907, 0.3787, 0.4567]],\n",
       "\n",
       "         [[0.9757, 0.1228, 0.8022],\n",
       "          [0.7647, 0.1846, 0.1259]],\n",
       "\n",
       "         [[0.4003, 0.5262, 0.7181],\n",
       "          [0.4864, 0.8433, 0.5089]]],\n",
       "\n",
       "\n",
       "        [[[0.8257, 0.9727, 0.1774],\n",
       "          [0.8441, 0.9851, 0.5339]],\n",
       "\n",
       "         [[0.0414, 0.5287, 0.4261],\n",
       "          [0.1476, 0.5123, 0.7263]],\n",
       "\n",
       "         [[0.8291, 0.4006, 0.2841],\n",
       "          [0.9272, 0.7009, 0.0623]]],\n",
       "\n",
       "\n",
       "        [[[0.7812, 0.2759, 0.0435],\n",
       "          [0.5987, 0.7608, 0.6836]],\n",
       "\n",
       "         [[0.0784, 0.8600, 0.8073],\n",
       "          [0.6366, 0.0255, 0.3495]],\n",
       "\n",
       "         [[0.1528, 0.8993, 0.2762],\n",
       "          [0.1313, 0.1862, 0.1518]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test =torch.rand(8, 3, 128, 128)\n",
    "torch.rand(8,3,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dfc9e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files into data_dict .................\n",
      "01\n",
      "02\n",
      "03\n",
      "04\n",
      "05\n",
      "06\n",
      "07\n",
      "08\n",
      "09\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "Loaded!!!!!\n"
     ]
    }
   ],
   "source": [
    "num_testsub = 32\n",
    "num_classes = 4\n",
    "num_channel = 40\n",
    "num_datapoints = 8064\n",
    "num_trials = 40\n",
    "sampling_rate = 128 # 128Hz as given in the data\n",
    "\n",
    "# Loads the saved numpy files that are saved into a dictionary\n",
    "def loadfiles():\n",
    "    data_dict = {}\n",
    "    print(\"Loading files into data_dict .................\")\n",
    "    for i in range(num_testsub):\n",
    "                    if i < 10:\n",
    "                        name = '%0*d' % (2,i+1)\n",
    "                    else:\n",
    "                        name = i+1\n",
    "                    fname = 'data/data_prepared/data/'+str(name) +'.npy'\n",
    "    #                 print(fname)\n",
    "                    data_dict[\"sub%s\" %name] = np.load(fname)\n",
    "                    print(name)\n",
    "    print(\"Loaded!!!!!\") \n",
    "    return data_dict\n",
    "data_dict = loadfiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84bf1c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5746088869762134\n",
      "0.5746088869762134\n",
      "0.5746088869762134\n",
      "-0.048606218951412566\n",
      "0.05329227715616479\n",
      "-0.02421026177223174\n"
     ]
    }
   ],
   "source": [
    "print(data_dict['sub05'][1][12][98])\n",
    "print(data_dict['sub32'][1][12][98])\n",
    "print(data_dict['sub31'][1][12][98])\n",
    "print(data_dict['sub01'][1][0][0])\n",
    "print(data_dict['sub01'][2][0][0])\n",
    "print(data_dict['sub01'][3][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401cba23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
